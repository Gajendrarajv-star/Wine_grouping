# -*- coding: utf-8 -*-
"""wine1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/118ng40bBrgk2eGcVQeJeVoJgTG99PG-u
"""

# Step 1: Import all necessary libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

# Step 2: Load the dataset
# Assuming the CSV file is already available
data_path = "Data (2).csv"
df = pd.read_csv(data_path)

# Step 3: Explore the dataset
print("First 5 rows of the dataset:")
print(df.head())
print("\nDataset info:")
print(df.info())

# Step 4: Handle Missing Values (if any)
print("\nMissing values in each column:")
print(df.isnull().sum())

# If there are any missing values, you can either fill or drop them. Here, we'll use mean imputation.
df.fillna(df.mean(), inplace=True)
print("\nAfter handling missing values (if any):")
print(df.isnull().sum())

# Step 5: Check for any duplicates
print("\nNumber of duplicate rows: ", df.duplicated().sum())

# Remove duplicates if they exist
df.drop_duplicates(inplace=True)
print("\nShape of data after removing duplicates: ", df.shape)

# Step 6: Check for outliers using boxplots
plt.figure(figsize=(15, 8))
sns.boxplot(data=df)
plt.title("Boxplot to Detect Outliers")
plt.xticks(rotation=90)
plt.show()

# You can handle outliers by capping, flooring, or removing them. Here's how to cap outliers using IQR.
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Capping outliers
df = df.clip(lower=lower_bound, upper=upper_bound, axis=1)

# Step 7: Data Preprocessing - Feature scaling (StandardScaler & MinMaxScaler)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(df)

# We can also try MinMaxScaler
min_max_scaler = MinMaxScaler()
data_normalized = min_max_scaler.fit_transform(df)

print("\nFirst 5 rows of the standardized data:")
print(pd.DataFrame(data_scaled, columns=df.columns).head())

print("\nFirst 5 rows of the normalized data (MinMax Scaling):")
print(pd.DataFrame(data_normalized, columns=df.columns).head())

# Step 8: Define a function to run KMeans and evaluate using multiple metrics
def run_kmeans(data, n_clusters):
    """Run KMeans and return model, labels, and evaluation metrics."""
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(data)

    # Evaluation metrics
    silhouette_avg = silhouette_score(data, labels)
    davies_bouldin = davies_bouldin_score(data, labels)
    calinski_harabasz = calinski_harabasz_score(data, labels)

    print(f"\nFor n_clusters = {n_clusters}:")
    print(f"Silhouette Score: {silhouette_avg}")
    print(f"Davies-Bouldin Index: {davies_bouldin}")
    print(f"Calinski-Harabasz Score: {calinski_harabasz}")

    return kmeans, labels, silhouette_avg, davies_bouldin, calinski_harabasz

# Step 9: Try different numbers of clusters and evaluate
n_clusters_options = [2, 3, 4, 5, 6, 7, 8, 9, 10]
silhouette_scores = []
davies_bouldin_scores = []
calinski_harabasz_scores = []

for n_clusters in n_clusters_options:
    _, labels, silhouette, davies_bouldin, calinski_harabasz = run_kmeans(data_scaled, n_clusters)
    silhouette_scores.append(silhouette)
    davies_bouldin_scores.append(davies_bouldin)
    calinski_harabasz_scores.append(calinski_harabasz)

# Step 10: Plot silhouette scores to find the optimal number of clusters
plt.figure(figsize=(10, 6))
plt.plot(n_clusters_options, silhouette_scores, marker='o', label='Silhouette Score')
plt.plot(n_clusters_options, davies_bouldin_scores, marker='o', label='Davies-Bouldin Index')
plt.plot(n_clusters_options, calinski_harabasz_scores, marker='o', label='Calinski-Harabasz Score')
plt.title('Evaluation Scores for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Scores')
plt.legend()
plt.grid(True)
plt.show()

# Step 11: Choose the best number of clusters based on evaluation metrics
optimal_clusters = n_clusters_options[np.argmax(silhouette_scores)]
print(f"\nOptimal number of clusters based on silhouette score: {optimal_clusters}")

# Step 12: Run KMeans with optimal number of clusters
kmeans, final_labels, _, _, _ = run_kmeans(data_scaled, optimal_clusters)

# Step 13: Add cluster labels to the original dataframe
df['Cluster'] = final_labels

print("\nFirst 5 rows of the dataset with cluster labels:")
print(df.head())

# Step 14: Visualize the clusters using PCA for dimensionality reduction
pca = PCA(n_components=2)
pca_data = pca.fit_transform(data_scaled)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=df['Cluster'], palette='Set1')
plt.title(f'Clusters Visualization with {optimal_clusters} Clusters (PCA)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# Step 15: Evaluate clusters - cluster sizes
cluster_sizes = df['Cluster'].value_counts()
print("\nCluster sizes:")
print(cluster_sizes)

# Step 16: Evaluate clusters - Mean values of features in each cluster
cluster_means = df.groupby('Cluster').mean()
print("\nMean values of features in each cluster:")
print(cluster_means)

# Step 17: Visualize cluster means for each feature
plt.figure(figsize=(12, 8))
sns.heatmap(cluster_means.T, annot=True, cmap='coolwarm')
plt.title(f'Heatmap of Feature Means for {optimal_clusters} Clusters')
plt.show()

# Step 18: Additional Cluster Evaluation - Distribution of clusters
plt.figure(figsize=(10, 6))
sns.countplot(x='Cluster', data=df, palette='Set2')
plt.title(f'Distribution of Clusters (K = {optimal_clusters})')
plt.show()

# Step 19: Conclusion - Summarize insights from clustering
print(f"\nSummary of clustering with {optimal_clusters} clusters:")
print("- Cluster sizes:")
print(cluster_sizes)
print("\n- Mean values of features in each cluster:")
print(cluster_means)

"""This result shows the outcome of clustering the wine dataset into 3 clusters (groups), which is a way of grouping similar wines together based on their features. Let's break it down step by step:

1. Cluster Sizes:
Cluster 0 contains 65 wines.
Cluster 2 contains 61 wines.
Cluster 1 contains 51 wines.
Each number represents how many wines were grouped into each of the 3 clusters. The wines in each cluster are more similar to each other based on their characteristics compared to wines in other clusters.

2. Mean Values of Features in Each Cluster:
For each cluster, the table shows the average values of different features (like alcohol content, acidity, ash content, etc.) that describe the wines in that cluster. These features are likely attributes of the wine, like alcohol content, ash levels, color intensity, etc. Here's what the averages tell us:

Cluster 0 (65 wines):
The wines in this group have an average alcohol content of 12.25, which is the lowest compared to other clusters.
The wines in this cluster also have moderate ash content (around 1.89), color intensity (2.97), and proline levels (510.17).
Cluster 1 (51 wines):
The wines in this group tend to have the highest values for many features:
The average alcohol content is 13.13, higher than Cluster 0 but lower than Cluster 2.
They have the highest proline levels (619.06).
These wines also have the highest ash content (3.29), but their color intensity is the highest (7.17), indicating these wines might be darker or richer in color.
Cluster 2 (61 wines):
Wines in this cluster have the highest alcohol content (13.67) on average.
This group also has a high color intensity (5.45), but it's lower than Cluster 1.
Wines in Cluster 2 have the highest proline level (1100.80), which is a distinctive characteristic of these wines compared to the other clusters.
Key Insights:
Cluster 0 wines are generally lower in alcohol content and proline levels, and their color intensity is moderate.
Cluster 1 wines are characterized by their high ash content, rich color intensity, and moderately high proline levels.
Cluster 2 wines have high alcohol content and very high proline levels, making them stand out from the others.
In simple terms, the clustering algorithm has grouped the wines based on similarities in their chemical properties, and each cluster represents a group of wines that share similar characteristics.
"""

# Final words on cluster evaluation
print("\nAdditional evaluation insights:")
print(f"- The silhouette score provides a measure of how similar an object is to its own cluster compared to other clusters.")
print(f"- The Davies-Bouldin index represents the ratio of within-cluster distances to between-cluster distances.")
print(f"- The Calinski-Harabasz score represents the ratio of the sum of between-cluster dispersion to within-cluster dispersion.")







